{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "877d7eb6",
   "metadata": {},
   "source": [
    "# üöÄ Export and quantize pretrained classification models for Arduino Nicla Vision deployment "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6362b77f",
   "metadata": {},
   "source": [
    "## Install FocoosAI SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99d5d2e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mUsing Python 3.11.12 environment at: /Users/u464645/Documents/projects/hackatons/project/sample/.venv\u001b[0m\n",
      "\u001b[2K   \u001b[36m\u001b[1mUpdating\u001b[0m\u001b[39m https://github.com/FocoosAI/focoos.git (\u001b[2mHEAD\u001b[0m)          \u001b[0m\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mUpdating\u001b[0m\u001b[39m https://github.com/FocoosAI/focoos.git (\u001b[2mHEAD\u001b[0m)  \u001b[0m\u001b[1A\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mUpdating\u001b[0m\u001b[39m https://github.com/FocoosAI/focoos.git (\u001b[2mHEAD\u001b[0m)  \u001b[0m\u001b[1A\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mUpdating\u001b[0m\u001b[39m https://github.com/FocoosAI/focoos.git (\u001b[2mHEAD\u001b[0m)  \u001b[0m\u001b[1A\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mUpdating\u001b[0m\u001b[39m https://github.com/FocoosAI/focoos.git (\u001b[2mHEAD\u001b[0m)  \u001b[0m\u001b[1A\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mUpdating\u001b[0m\u001b[39m https://github.com/FocoosAI/focoos.git (\u001b[2mHEAD\u001b[0m)  \u001b[0m\u001b[1A\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mUpdating\u001b[0m\u001b[39m https://github.com/FocoosAI/focoos.git (\u001b[2mHEAD\u001b[0m)  \u001b[0m\u001b[1A\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mUpdating\u001b[0m\u001b[39m https://github.com/FocoosAI/focoos.git (\u001b[2mHEAD\u001b[0m)  \u001b[0m\u001b[1A\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mUpdating\u001b[0m\u001b[39m https://github.com/FocoosAI/focoos.git (\u001b[2mHEAD\u001b[0m)  \u001b[0m\u001b[1A\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mUpdating\u001b[0m\u001b[39m https://github.com/FocoosAI/focoos.git (\u001b[2mHEAD\u001b[0m)  \u001b[0m\u001b[1A\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mUpdating\u001b[0m\u001b[39m https://github.com/FocoosAI/focoos.git (\u001b[2mHEAD\u001b[0m)  \u001b[0m\u001b[1A\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mUpdating\u001b[0m\u001b[39m https://github.com/FocoosAI/focoos.git (\u001b[2mHEAD\u001b[0m)  \u001b[0m\u001b[1A\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mUpdating\u001b[0m\u001b[39m https://github.com/FocoosAI/focoos.git (\u001b[2mHEAD\u001b[0m)  \u001b[0m\u001b[1A\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mUpdating\u001b[0m\u001b[39m https://github.com/FocoosAI/focoos.git (\u001b[2mHEAD\u001b[0m)  \u001b[0m\u001b[1A\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mUpdating\u001b[0m\u001b[39m https://github.com/FocoosAI/focoos.git (\u001b[2mHEAD\u001b[0m)  \u001b[0m\u001b[1A\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mUpdating\u001b[0m\u001b[39m https://github.com/FocoosAI/focoos.git (\u001b[2mHEAD\u001b[0m)  \u001b[0m\u001b[1A\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mUpdating\u001b[0m\u001b[39m https://github.com/FocoosAI/focoos.git (\u001b[2mHEAD\u001b[0m)  \u001b[0m\u001b[1A\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mUpdating\u001b[0m\u001b[39m https://github.com/FocoosAI/focoos.git (\u001b[2mHEAD\u001b[0m)  \u001b[0m\u001b[1A\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mUpdating\u001b[0m\u001b[39m https://github.com/FocoosAI/focoos.git (\u001b[2mHEAD\u001b[0m)  \u001b[0m\u001b[1A\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mUpdating\u001b[0m\u001b[39m https://github.com/FocoosAI/focoos.git (\u001b[2mHEAD\u001b[0m)  \u001b[0m\u001b[1A\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mUpdating\u001b[0m\u001b[39m https://github.com/FocoosAI/focoos.git (\u001b[2mHEAD\u001b[0m)  \u001b[0m\u001b[1A\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mUpdating\u001b[0m\u001b[39m https://github.com/FocoosAI/focoos.git (\u001b[2mHEAD\u001b[0m)  \u001b[0m\u001b[1A\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mUpdating\u001b[0m\u001b[39m https://github.com/FocoosAI/focoos.git (\u001b[2mHEAD\u001b[0m)  \u001b[0m\u001b[1A\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mUpdating\u001b[0m\u001b[39m https://github.com/FocoosAI/focoos.git (\u001b[2mHEAD\u001b[0m)  \u001b[0m\u001b[1A\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mUpdating\u001b[0m\u001b[39m https://github.com/FocoosAI/focoos.git (\u001b[2mHEAD\u001b[0m)  \u001b[0m\u001b[1A\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mUpdating\u001b[0m\u001b[39m https://github.com/FocoosAI/focoos.git (\u001b[2mHEAD\u001b[0m)  \u001b[0m\u001b[1A\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mUpdating\u001b[0m\u001b[39m https://github.com/FocoosAI/focoos.git (\u001b[2mHEAD\u001b[0m)  \u001b[0m\u001b[1A\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mUpdating\u001b[0m\u001b[39m https://github.com/FocoosAI/focoos.git (\u001b[2mHEAD\u001b[0m)  \u001b[0m\u001b[1A\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mUpdating\u001b[0m\u001b[39m https://github.com/FocoosAI/focoos.git (\u001b[2mHEAD\u001b[0m)  \u001b[0m\u001b[1A\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mUpdating\u001b[0m\u001b[39m https://github.com/FocoosAI/focoos.git (\u001b[2mHEAD\u001b[0m)  \u001b[0m\u001b[1A\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mUpdating\u001b[0m\u001b[39m https://github.com/FocoosAI/focoos.git (\u001b[2mHEAD\u001b[0m)  \u001b[0m\u001b[1A\n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mUpdating\u001b[0m\u001b[39m https://github.com/FocoosAI/focoos.git (\u001b[2mHEAD\u001b[0m)  \u001b[0m\u001b[1A\n",
      "\u001b[2K\u001b[1A    \u001b[32m\u001b[1mUpdated\u001b[0m\u001b[39m https://github.com/FocoosAI/focoos.git (\u001b[2m0136006e3c4d38d30557146987339ed7\n",
      "\u001b[2K\u001b[37m‚†π\u001b[0m \u001b[2mfocoos==0.22.0                                                                \u001b[0m  \u001b[31m√ó\u001b[0m Failed to fetch: `https://pypi.org/simple/colorama/`\n",
      "\u001b[31m  ‚îú‚îÄ‚ñ∂ \u001b[0mRequest failed after 3 retries\n",
      "\u001b[31m  ‚îú‚îÄ‚ñ∂ \u001b[0merror sending request for url (https://pypi.org/simple/colorama/)\n",
      "\u001b[31m  ‚îú‚îÄ‚ñ∂ \u001b[0mclient error (Connect)\n",
      "\u001b[31m  ‚ï∞‚îÄ‚ñ∂ \u001b[0minvalid peer certificate: UnknownIssuer\n",
      "\u001b[36m  help: \u001b[0mConsider enabling use of system TLS certificates with the\n",
      "        `\u001b[32m--native-tls\u001b[39m` command-line flag\n"
     ]
    }
   ],
   "source": [
    "!uv pip install 'focoos[onnx-cpu] @ git+https://github.com/FocoosAI/focoos.git'\n",
    "\n",
    "# you can use [onnx] extra dependencies if you are in a GPU or Colab environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453e2d41",
   "metadata": {},
   "source": [
    "## Get Pretrained Model\n",
    "\n",
    "FocoosAI offers three pretrained classification models in different sizes:\n",
    "\n",
    "- fai-cls-n-coco  (nano, optimized for Arduino Nicla Vision) \n",
    "- fai-cls-s-coco  (small)\n",
    "- fai-cls-m-coco  (medium)\n",
    "\n",
    "all models are trained on coco dataset at 224px resolution, **but for Nicla vision we suggest exporting them to 96px.**\n",
    "\n",
    "Choose the model size that best fits your accuracy and efficiency needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d6094769",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;32m[10/05 02:33][INFO][HUB]: Currently logged as: frigato.luca97@gmail.com environment: https://api.focoos.ai/v0\u001b[0m\n",
      "\u001b[1;32m[10/05 02:33][INFO][HUB]: üì• Model already downloaded\u001b[0m\n",
      "\u001b[1;32m[10/05 02:33][INFO][ModelManager]: üì• Loading model info from cache: /Users/u464645/FocoosAI/models/f73c51dfb3bd422f/model_info.json\u001b[0m\n",
      "\u001b[1;35m[10/05 02:33][WARNING][Backbone]: Layers must be [2, 2, 2] if size is nano, provided [4, 5, 3] not used.\u001b[0m\n",
      "\u001b[1;35m[10/05 02:33][WARNING][Backbone]: Base must be 32 if size is nano, provided 64 not used.\u001b[0m\n",
      "\u001b[1;35m[10/05 02:33][WARNING][FocoosModel]: Unable to use CUDA\u001b[0m\n",
      "\u001b[1;32m[10/05 02:33][INFO][FocoosModel]: Loading weights from local path: /Users/u464645/FocoosAI/models/f73c51dfb3bd422f/model_final.pth\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1 background, 1 humans\n",
      "Latency: imload 8ms, preprocess 3ms, inference 22ms, postprocess 0ms, annotate 2ms, total 35ms\n",
      "FocoosDet(bbox=None, conf=0.609123706817627, cls_id=1, label=humans, mask=None, keypoints=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "from unittest import result\n",
    "\n",
    "\n",
    "from focoos import FocoosHUB, ModelManager\n",
    "\n",
    "hub = FocoosHUB(api_key=\"c7ef8380320c421792425668205fa8fa\")\n",
    "\n",
    "ref = 'f73c51dfb3bd422f'\n",
    "model = ModelManager.get(f\"hub://{ref}\", hub=hub)\n",
    "\n",
    "im = '/Users/u464645/Documents/projects/hackatons/project/sample/webcam_captures/capture_1_20251005_014108.jpg'\n",
    "result = model.infer(image=im, threshold=0.4, annotate=True)\n",
    "print(result.detections[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1a3e20",
   "metadata": {},
   "source": [
    "## Export as optimized ONNX for edge deployment\n",
    "\n",
    "For edge deployment, we need to export model to more portable runtime, like onnxruntime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "134d08c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;32m[10/05 02:33][INFO][FocoosModel]: üîß Export Device: cpu\u001b[0m\n",
      "\u001b[1;32m[10/05 02:33][INFO][FocoosModel]: üöÄ Exporting ONNX model with Optimum..\u001b[0m\n",
      "\u001b[1;32m[10/05 02:33][INFO][FocoosModel]: üìä Nodes in graph: 47\u001b[0m\n",
      "\u001b[1;32m[10/05 02:33][INFO][FocoosModel]: ‚úÖ ONNX export completed \u001b[0m\n",
      "\u001b[1;32m[10/05 02:33][INFO][FocoosModel]: üîß Applying ONNX Simplify: Run Optimum graph optimizations...\u001b[0m\n",
      "\u001b[1;32m[10/05 02:33][INFO][onnx_model_bert]: opset version: 18\u001b[0m\n",
      "\u001b[1;32m[10/05 02:33][INFO][onnx_model]: Sort graphs in topological order\u001b[0m\n",
      "\u001b[1;32m[10/05 02:33][INFO][onnx_model]: Model saved to /Users/u464645/Documents/projects/hackatons/project/sample/export/f73c51dfb3bd422f/model_optimized.onnx\u001b[0m\n",
      "\u001b[1;32m[10/05 02:33][INFO][FocoosModel]: üìä After ONNX Runtime optimizations: 47 nodes in graph\u001b[0m\n",
      "\u001b[1;32m[10/05 02:33][INFO][FocoosModel]: üìà Reduction: ~0.0% nodes removed!\u001b[0m\n",
      "\u001b[1;32m[10/05 02:33][INFO][FocoosModel]: ‚úÖ Onnx model successfully simplified.\u001b[0m\n",
      "\u001b[1;32m[10/05 02:33][INFO][FocoosModel]: ‚úÖ Exported ExportFormat.ONNX  model to /Users/u464645/Documents/projects/hackatons/project/sample/export/f73c51dfb3bd422f/model.onnx\u001b[0m\n",
      "\u001b[1;33m[10/05 02:33][DEBUG][InferModel]: Runtime type: onnx_cpu, Loading model from /Users/u464645/Documents/projects/hackatons/project/sample/export/f73c51dfb3bd422f/model.onnx..\u001b[0m\n",
      "\u001b[1;33m[10/05 02:33][DEBUG][ONNXRuntime]: üîß [onnxruntime device] CPU\u001b[0m\n",
      "\u001b[1;33m[10/05 02:33][DEBUG][ONNXRuntime]: Available providers:['CoreMLExecutionProvider', 'AzureExecutionProvider', 'CPUExecutionProvider']\u001b[0m\n",
      "\u001b[1;32m[10/05 02:33][INFO][ONNXRuntime]:  using: CPUExecutionProvider\u001b[0m\n",
      "\u001b[1;32m[10/05 02:33][INFO][ONNXRuntime]: ‚è±Ô∏è Warming up model model on CPUExecutionProvider, size: 96x96..\u001b[0m\n",
      "\u001b[1;35m[10/05 02:33][WARNING][focoos.utils.system]: nvidia-smi command not found: [Errno 2] No such file or directory: 'nvidia-smi'\u001b[0m\n",
      "\u001b[1;32m[10/05 02:33][INFO][ONNXRuntime]: ‚è±Ô∏è Benchmarking latency on arm, size: (96, 96)..\u001b[0m\n",
      "\u001b[1;32m[10/05 02:33][INFO][ONNXRuntime]: üî• FPS: 2624 Mean latency: 0.381 ms \u001b[0m\n",
      "UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1 background, 1 humans\n",
      "Latency: imload 8ms, preprocess 3ms, inference 19ms, postprocess 0ms, annotate 2ms, total 32ms\n",
      "FocoosDet(bbox=None, conf=0.609123706817627, cls_id=1, label=humans, mask=None, keypoints=None)\n",
      "FocoosDetections(detections=[FocoosDet(bbox=None, conf=0.6248612403869629, cls_id=0, label=background, mask=None, keypoints=None), FocoosDet(bbox=None, conf=0.609123706817627, cls_id=1, label=humans, mask=None, keypoints=None)], image=hidden, latency=InferLatency(imload=0.008, preprocess=0.003, inference=0.019, postprocess=0.0, annotate=0.002))\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from focoos import ASSETS_DIR, RuntimeType\n",
    "\n",
    "image_size = 96  # 96px input size\n",
    "\n",
    "\n",
    "def find_workspace_root():\n",
    "    \"\"\"Find the workspace root directory.\"\"\"\n",
    "    current_path = Path.cwd()\n",
    "    \n",
    "    # Look for workspace indicators\n",
    "    for path in [current_path] + list(current_path.parents):\n",
    "        if (path / \"pyproject.toml\").exists() or (path / \".git\").exists() or (path / \"README.md\").exists():\n",
    "            return path\n",
    "    \n",
    "    # Fallback to current directory\n",
    "    return current_path\n",
    "\n",
    "# Add project root to path for imports\n",
    "project_root = find_workspace_root()\n",
    "\n",
    "out_dir = project_root / \"export\"\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "exported_model = model.export(\n",
    "    runtime_type=RuntimeType.ONNX_CPU,  # optimized for edge or cpu\n",
    "    image_size=image_size,\n",
    "    dynamic_axes=False,  # quantization need static axes!\n",
    "    simplify_onnx=True,  # simplify and optimize onnx model graph\n",
    "    onnx_opset=18,\n",
    "    out_dir=os.path.join(out_dir, ref)\n",
    ")  # save to models dir\n",
    "\n",
    "# benchmark onnx model\n",
    "exported_model.benchmark(iterations=100)\n",
    "\n",
    "# test onnx model\n",
    "im = '/Users/u464645/Documents/projects/hackatons/project/sample/webcam_captures/capture_1_20251005_014108.jpg'\n",
    "result = model.infer(image=im, threshold=0.5, annotate=True)\n",
    "print(result.detections[1])\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d7efca",
   "metadata": {},
   "source": [
    "## Quantize exported model to int8 (or uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e178456c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created calibration folder with 100 images\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "# Create a temporary flat calibration folder\n",
    "calibration_temp_dir = project_root / \"temp_calibration\"\n",
    "calibration_temp_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Clear existing files\n",
    "for file in calibration_temp_dir.glob(\"*\"):\n",
    "    if file.is_file():\n",
    "        file.unlink()\n",
    "\n",
    "# Copy a subset of images from both classes to the temp folder\n",
    "val_dir = Path(\"/Users/u464645/Documents/projects/hackatons/project/sample/datasets/human_identification_balanced/val/\")\n",
    "\n",
    "# Take first 50 images from each class for calibration (adjust as needed)\n",
    "for class_dir in val_dir.iterdir():\n",
    "    if class_dir.is_dir():\n",
    "        images = list(class_dir.glob(\"*.jpg\"))[:50]  # Take first 50 images\n",
    "        for i, img in enumerate(images):\n",
    "            dest_name = f\"{class_dir.name}_{i:03d}_{img.name}\"\n",
    "            shutil.copy2(img, calibration_temp_dir / dest_name)\n",
    "\n",
    "print(f\"Created calibration folder with {len(list(calibration_temp_dir.glob('*.jpg')))} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2b52a687",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;32m[10/05 02:34][INFO][OnnxQuantizer]: Setting up data reader with calibration images: /Users/u464645/Documents/projects/hackatons/project/sample/temp_calibration\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: 96, 96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;32m[10/05 02:34][INFO][onnxruntime.quantization.shape_inference]: Performing symbolic shape inference...\u001b[0m\n",
      "\u001b[1;32m[10/05 02:34][INFO][OnnxQuantizer]: üîß Quantizing model from /Users/u464645/Documents/projects/hackatons/project/sample/export/f73c51dfb3bd422f/model.onnx to /Users/u464645/Documents/projects/hackatons/project/sample/export/f73c51dfb3bd422f/model_int8.onnx\u001b[0m\n",
      "\u001b[1;32m[10/05 02:34][INFO][OnnxQuantizer]: ‚úÖ Quantized model saved successfully to /Users/u464645/Documents/projects/hackatons/project/sample/export/f73c51dfb3bd422f/model_int8.onnx\u001b[0m\n",
      "\u001b[1;32m[10/05 02:34][INFO][OnnxQuantizer]: ================== BENCHMARKING FP32 MODEL ==================\u001b[0m\n",
      "\u001b[1;33m[10/05 02:34][DEBUG][InferModel]: Runtime type: onnx_cpu, Loading model from /Users/u464645/Documents/projects/hackatons/project/sample/export/f73c51dfb3bd422f/model.onnx..\u001b[0m\n",
      "\u001b[1;33m[10/05 02:34][DEBUG][ONNXRuntime]: üîß [onnxruntime device] CPU\u001b[0m\n",
      "\u001b[1;33m[10/05 02:34][DEBUG][ONNXRuntime]: Available providers:['CoreMLExecutionProvider', 'AzureExecutionProvider', 'CPUExecutionProvider']\u001b[0m\n",
      "\u001b[1;32m[10/05 02:34][INFO][ONNXRuntime]:  using: CPUExecutionProvider\u001b[0m\n",
      "\u001b[1;32m[10/05 02:34][INFO][ONNXRuntime]: ‚è±Ô∏è Warming up model model on CPUExecutionProvider, size: 96x96..\u001b[0m\n",
      "\u001b[1;35m[10/05 02:34][WARNING][focoos.utils.system]: nvidia-smi command not found: [Errno 2] No such file or directory: 'nvidia-smi'\u001b[0m\n",
      "\u001b[1;32m[10/05 02:34][INFO][ONNXRuntime]: ‚è±Ô∏è Benchmarking latency on arm, size: (96, 96)..\u001b[0m\n",
      "\u001b[1;32m[10/05 02:34][INFO][ONNXRuntime]: üî• FPS: 2552 Mean latency: 0.392 ms \u001b[0m\n",
      "\u001b[1;32m[10/05 02:34][INFO][OnnxQuantizer]: ================================================================\u001b[0m\n",
      "\u001b[1;32m[10/05 02:34][INFO][OnnxQuantizer]: ================== BENCHMARKING INT8 MODEL ==================\u001b[0m\n",
      "\u001b[1;33m[10/05 02:34][DEBUG][InferModel]: Runtime type: onnx_cpu, Loading model from /Users/u464645/Documents/projects/hackatons/project/sample/export/f73c51dfb3bd422f/model_int8.onnx..\u001b[0m\n",
      "\u001b[1;33m[10/05 02:34][DEBUG][ONNXRuntime]: üîß [onnxruntime device] CPU\u001b[0m\n",
      "\u001b[1;33m[10/05 02:34][DEBUG][ONNXRuntime]: Available providers:['CoreMLExecutionProvider', 'AzureExecutionProvider', 'CPUExecutionProvider']\u001b[0m\n",
      "\u001b[1;32m[10/05 02:34][INFO][ONNXRuntime]:  using: CPUExecutionProvider\u001b[0m\n",
      "\u001b[1;32m[10/05 02:34][INFO][ONNXRuntime]: ‚è±Ô∏è Warming up model model_int8 on CPUExecutionProvider, size: 96x96..\u001b[0m\n",
      "\u001b[1;35m[10/05 02:34][WARNING][focoos.utils.system]: nvidia-smi command not found: [Errno 2] No such file or directory: 'nvidia-smi'\u001b[0m\n",
      "\u001b[1;32m[10/05 02:34][INFO][ONNXRuntime]: ‚è±Ô∏è Benchmarking latency on arm, size: (96, 96)..\u001b[0m\n",
      "\u001b[1;32m[10/05 02:34][INFO][ONNXRuntime]: üî• FPS: 5037 Mean latency: 0.199 ms \u001b[0m\n",
      "\u001b[1;32m[10/05 02:34][INFO][OnnxQuantizer]: ================================================================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from focoos.infer.quantizer import OnnxQuantizer, QuantizationCfg\n",
    "\n",
    "quantization_cfg = QuantizationCfg(\n",
    "    size=image_size,  # input size: must be same as exported model\n",
    "    calibration_images_folder=str(calibration_temp_dir),  # Use the temporary flat calibration folder\n",
    "    # to use the dataset validation split on which the model was trained.\n",
    "    format=\"QO\",  # QO (QOperator): All the quantized operators have their own ONNX definitions, like QLinearConv, MatMulInteger etc.\n",
    "    # QDQ (Quantize-DeQuantize): inserts DeQuantizeLinear(QuantizeLinear(tensor)) between the original operators to simulate the quantization and dequantization process.\n",
    "    per_channel=False,  # Per-channel quantization: each channel has its own scale/zero-point ‚Üí more accurate,\n",
    "    # especially for convolutions, at the cost of extra memory and computation.\n",
    "    normalize_images=True,  # normalize images during preprocessing: some models have normalization outside of model forward\n",
    ")\n",
    "\n",
    "quantizer = OnnxQuantizer(input_model_path=exported_model.model_path, cfg=quantization_cfg)\n",
    "model_path = quantizer.quantize(\n",
    "    benchmark=True  # benchmark bot fp32 and int8 models\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8249ac23",
   "metadata": {},
   "source": [
    "## Inference with quantized model on cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9ead8c9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;33m[10/05 02:34][DEBUG][InferModel]: Runtime type: onnx_cpu, Loading model from /Users/u464645/Documents/projects/hackatons/project/sample/export/f73c51dfb3bd422f/model_int8.onnx..\u001b[0m\n",
      "\u001b[1;33m[10/05 02:34][DEBUG][ONNXRuntime]: üîß [onnxruntime device] CPU\u001b[0m\n",
      "\u001b[1;33m[10/05 02:34][DEBUG][ONNXRuntime]: Available providers:['CoreMLExecutionProvider', 'AzureExecutionProvider', 'CPUExecutionProvider']\u001b[0m\n",
      "\u001b[1;32m[10/05 02:34][INFO][ONNXRuntime]:  using: CPUExecutionProvider\u001b[0m\n",
      "\u001b[1;32m[10/05 02:34][INFO][ONNXRuntime]: ‚è±Ô∏è Warming up model model_int8 on CPUExecutionProvider, size: 96x96..\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1 background, 1 humans\n",
      "Latency: imload 9ms, preprocess 1ms, inference 0ms, postprocess 0ms, annotate 2ms, total 12ms\n",
      "\n",
      "1 background, 1 humans\n",
      "Latency: imload 8ms, preprocess 1ms, inference 22ms, postprocess 0ms, annotate 1ms, total 32ms\n",
      "FocoosDet(bbox=None, conf=0.609123706817627, cls_id=1, label=humans, mask=None, keypoints=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n"
     ]
    }
   ],
   "source": [
    "from focoos import InferModel\n",
    "\n",
    "quantized_model = InferModel(model_path, runtime_type=RuntimeType.ONNX_CPU)\n",
    "\n",
    "res = quantized_model.infer(im, annotate=True)\n",
    "Image.fromarray(res.image)\n",
    "result = model.infer(image=im, threshold=0.5, annotate=True)\n",
    "print(result.detections[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1211e7af",
   "metadata": {},
   "source": [
    "# Train a model from scratch with FocoosAI HUB\n",
    "üëã Welcome to FocoosAI!\n",
    "\n",
    "[Create your free account](https://app.focoos.ai/) on the FocoosAI platform and get access to:\n",
    "\n",
    "- **10 hours of GPU cloud training** \n",
    "- **5GB of cloud storage** for your datasets and models\n",
    "- **1000 cloud inference requests** to test your models\n",
    "- **Up to 20 models** to experiment with\n",
    "\n",
    "Start building amazing computer vision models today with our generous free tier!\n",
    "\n",
    "## Uplaod a dataset\n",
    "\n",
    "The first step in a computer vision pipeline always starts with data. \n",
    "You can use a dataset shared by FocoosAI or upload your own.\n",
    "\n",
    "You can also find some classification dataset on [Roboflow Universe](https://universe.roboflow.com/search?q=classification) or just create your own with some scraping!\n",
    "\n",
    "The supported classification dataset layout are:\n",
    "- **classification folder** (*'folder structure'* on roboflow):\n",
    "\n",
    "```bash\n",
    "root/\n",
    "    train/\n",
    "        cls1/\n",
    "            - img_1.jpg\n",
    "            - img_2.jpg\n",
    "        cls2/\n",
    "            - img_1.jpg\n",
    "            - img_2.jpg\n",
    "    valid/\n",
    "        cls1/\n",
    "            - img_1.jpg\n",
    "            - img_2.jpg\n",
    "        cls2/\n",
    "            img_1.jpg\n",
    "            img_2.jpg\n",
    "```\n",
    "- **roboflow_coco** (*'COCO'* on roboflow):\n",
    "```bash\n",
    "root/\n",
    "    train/\n",
    "        - _annotations.coco.json\n",
    "        - img_1.jpg\n",
    "        - img_2.jpg\n",
    "    valid/\n",
    "        - _annotations.coco.json\n",
    "        - img_3.jpg\n",
    "        - img_4.jpg\n",
    "```\n",
    "\n",
    "First, you need to compress your dataset in .zip format and upload on focoos platform in few clicks:\n",
    "\n",
    "- go to the dataset page and click on  **\"+ add dataset\"**, chouse your dataset and upload:\n",
    "\n",
    "![New Dataset](https://i.imgur.com/2zwPs8d.png)\n",
    "![Uploading](https://i.imgur.com/7GmwOzW.png)\n",
    "\n",
    "- Once the upload is finished, you can view a summary, preview, and statistics of your dataset:\n",
    "\n",
    "![Dataset Info](https://i.imgur.com/qXHFXtU.png) \n",
    "\n",
    "## Train a model\n",
    "- Now we can finally train a model with this dataset, just click on **\"Use for training a new model\"** and insert name and description about your model:\n",
    "\n",
    "![New Model](https://i.imgur.com/Io5f4E6.png)\n",
    "\n",
    "- Configure the hyperparameters, with a recommended batch size of 128 and a resolution of 96px, then initiate the training process:\n",
    "\n",
    "![Hyperparameters](https://i.imgur.com/NIb5Dq4.png)\n",
    "\n",
    "- While training, you can monitor the accuracy and loss metrics:\n",
    "\n",
    "![training](https://i.imgur.com/EX3BXti.png)\n",
    "\n",
    "- Once training is finished, you can perform inference directly in the browser to evaluate results:\n",
    "\n",
    "![Inference](https://i.imgur.com/xcHIWv0.png)\n",
    "\n",
    "- If you are not satisfied with the results, you can train another model with different hyperparameters and compare the models in the ‚Äúcompare models‚Äù section.\n",
    "\n",
    "- Otherwise, you can go to the code snippet section and copy the code to download it to the SDK:\n",
    "\n",
    "![Code Snippet](https://i.imgur.com/Ju7RgFn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80dc5151",
   "metadata": {},
   "source": [
    "## Export and quantize HUB trained models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83fd3e4b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a581efd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from focoos import FocoosHUB, ModelManager\n",
    "from focoos.infer.quantizer import OnnxQuantizer, QuantizationCfg\n",
    "\n",
    "hub = FocoosHUB(api_key=\"YOUR_API_KEY\")\n",
    "model = ModelManager.get(\"hub://YOUR_MODEL_REF\", hub=hub)\n",
    "\n",
    "exported_model = model.export(\n",
    "    runtime_type=RuntimeType.ONNX_CPU,\n",
    "    image_size=96,\n",
    "    dynamic_axes=False,\n",
    "    simplify_onnx=True,  # simplify and optimize onnx model graph\n",
    "    onnx_opset=18,\n",
    "    out_dir=os.path.join(\"export/\", \"my_hub_model\"),\n",
    ")\n",
    "\n",
    "\n",
    "quantization_cfg = QuantizationCfg(\n",
    "    size=image_size,  # input size: must be same as exported model\n",
    "    calibration_images_folder=str(\"/home/ubuntu/focoos/datasets/coco/val2017\"),  # Calibration images folder: It is strongly recommended\n",
    "    # to use the dataset validation split on which the model was trained.\n",
    "    # Here, for example, we will use the assets folder.\n",
    "    format=\"QO\",  # QO (QOperator): All the quantized operators have their own ONNX definitions, like QLinearConv, MatMulInteger etc.\n",
    "    # QDQ (Quantize-DeQuantize): inserts DeQuantizeLinear(QuantizeLinear(tensor)) between the original operators to simulate the quantization and dequantization process.\n",
    "    per_channel=False,  # Per-channel quantization: each channel has its own scale/zero-point ‚Üí more accurate,\n",
    "    # especially for convolutions, at the cost of extra memory and computation.\n",
    "    normalize_images=True,  # normalize images during preprocessing: some models have normalization outside of model forward\n",
    ")\n",
    "\n",
    "quantizer = OnnxQuantizer(input_model_path=exported_model.model_path, cfg=quantization_cfg)\n",
    "model_path = quantizer.quantize(\n",
    "    benchmark=True  # benchmark bot fp32 and int8 models\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "focoos-sample-app",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
